spark:内核



RDD:都做了什么？ rdd的创建、rdd的转换、rdd的缓存、rdd的行动，返回一个scala的标量

RDD的弹性是指什么？ RDD的数据可以动态的在内存和磁盘之间切换RDD的重试机制（避免由于网络等原因导致的任务失败）、动态的根据需求改变分区数（可以认为分区数就是并行度），可以通过血统机制进行RDD数据的恢复


RDD的默认分区数如何确定的：根据schedulerBackEnd父类方法defaultParallelism方法 读取的
“spark.default.parallelism”配置文件确定的如果没有没有配置的话 根据math.max(totalCoreCount.get(),2)确定的

一个task处理一个分区的数据


Rdd的运行机制：先从stage的划分说起（宽窄依赖关系），划分完形成了一个类似栈结构，最后划分的stage放在了栈顶，那么执行的时候就从前往后执行了

用groupbykey实现reducebykey的功能

val rdd = sc.makeRDD( 1 to 10 ).union(6 to 15)
val rdd1 = rdd.groupByKey().map(item = >(item._1,item._2.reduceleft(_+_))).collect

或者 
val rdd1 = rdd.groupByKey().map{case (x,y) => (x,y.sum)}.collect



求学生的平均成绩:(非常重要)
def combineByKey[C](createCombiner: V => C,mergeValue: (C, V) => C,mergeCombiners: (C, C) => C): RDD[(K, C)]

val rdd = sc.makeRDD(Array(("a",50),("b","30"),("a",15),("c",20)))


val rdd1 = rrd.combineByKey((_,1),(c:(Int,Int),v) => (c._1 + v,c._2 + 1),(c1:(Int,Int),c2:(Int,Int)) => (c1._1 + c2._1,c1._2 + c2._2))

解析三步如下:

createCombiner: V => C:
(_,1)
a(50,1),b(30,1),a(15,1),c(20,1)


mergeValue: (C, V) => C:
(c:(Int,Int),v) => (c._1 + v,c._2 + 1)

c._1 +v =  相同key值相加 50 + 15

c._2 +1 =  相同key的个数相加 1 +1



mergeCombiners: (C, C) => C

c2:(Int,Int)) => (c1._1 + c2._1,c1._2 + c2._2)

不同分区 相同key值的相加 和相同key的个数相加 组合成一个元组



aggregateBykey是combinekey的简化版：

zerovalue 相当于 combineBykey

val rdd1 = rrd.combineByKey((0,0),(c:(Int,Int),v) => (c._1 + v,c._2 + 1),(c1:(Int,Int),c2:(Int,Int)) => (c1._1 + c2._1,c1._2 + c2._2))

foldByKey又是  aggregateBykey的简化版


注意：countByKey是行动操作，并不是转换操作

spark目前分成hash分区（容易数据倾斜） 和 range分区（能保证数据均匀的，使用了水塘抽样算法）


cache相当于提供了memory_only级别的presit的内存存储，presit提供了多种的内存存储级别，常见的是memory_and_disk


在转换操作里使用累加器可能会出现值重复的情况，最好在行动操作里面使用累加器，在使用时不能修改累加器，只能访问

不使用广播变量会在executor里的每个分区都拷贝一份变量的副本，会造成大量的网络数据传输

广播变量一般用来分发只读的小数据集


DataFrame性能要比rdd性能高:
1、定制化内存管理:sparksql的所有数据以二进行形式(因为堆外内存，没有堆内内存的那些 方法区、方法栈只能序列化城2进制)，存在offheap

2、优化的执行计划:比如filter下推

DataFrame缺点：在编译器缺少类型检查，在运行期才检查

DataSet 具有类型安全检查的特点(DataFrame + caseClass)，并且DataSet具有编解码器，不用整个对象序列化，可以把单独的某个属性返回，大大的减少了序列化和反序列化的时间

SparkCore 是sparkcore之上的一个模块，主要用来处理结构化数据
spark默认的序列化是java的，2.0之后开始使用kyro


spark序列化:
1、分发给executor上的task
2、需要缓存的rdd
3、广播变量
4、shuffle过程中的缓存数据
5、使用receiver方式接收的流数据缓存
6、算子函数中使用的外部变量

spark优雅的退出:
使用yarn application kill applicationRDD 停止程序的缺点：
resourcemanager会给大概1s的时间让driver停止，但是如果在给定的时间内还没有提交完任务，resourcemanager
会强制杀死driver，会导致数据丢失

如何优雅的停止呢？
在main函数里创建线程 监控hdfs的某一个目录，当这个目录不存在的时候 我的spark streaming正常运行，当这个目录一旦被创建
在main函数的线程里调用streaming context的stop方法

sparkstreaming的checkpoint会check两种数据类型：

一部分：meta元数据，用于保存streaming计算逻辑的元数据：
1、streaming application的所有配置
2、dstream的一系列操作
3、提交的job，但还没有执行或未完成的batches

第二部分：数据（sparkstreaming已经处理完成的rdd数据）


什么时候启用checkpoint：
1）使用了有状态的transformation的操作比如：updatestatebykey，或者
reducebykeyandwindow操作（问老师为啥）
2）从意外中恢复driver，可以开启checkpoint

checkpoint机制：
当对某一个dstream开始执行checkpoint的时候，会传个时间间隔，每当程序执行到这个位置的时候都会把dstream数据落盘在checkpoint目录里
当执行一个checkpoint的时候会把streaming context的上下文全都落盘到checkpoint目录里
当checkpoint目录落盘数据的时候 会先序列化在存储,一旦反序列化如果代码修改了，对应的序列号就会变，找不到了，
所以应该先把hdfs上的checkpoint目录删除掉，在运行代码重新生成


Reciver是spark和kafka对接的 高阶api，一次从kafka里拉取一匹数据到executor的内存里，如果executor挂掉了就会数据丢失，所以
需要先写到磁盘的wal文件里，在进行后续处理(所以性能比较差)


sparkStreaming(微批次架构)和storm不一样，storm是针对一行数据，sparkstreaming是批次处理数据，粒度比storm大，但是吞吐量比storm也大

sparkstreaming：分为无状态操作和有状态操作，这里面的状态指的实际就是数据，由spark自己维护
sparkstreaming的抽象就是dstream（离散化流，可以理解成由多个rdd组成）

sparkstreaming背压机制：通过背压机制调节入水口的速度，数据流进来之后如果想转换成rdd，首先需要得到一个令牌，所以控制水流的速度，也就是控制令牌的生成时间，代码里：receiverRateController会一直评估数据的处理时间，还有数据的采集时间,如果发现数据的处理时间比数据的采集时间长很多，那么就会更新所有的令牌的生成，逐渐达到一个平衡时间


Dstream的输入源有：文件数据源、RDD队列、自定义数据源、kafka、flume

kafka 一个topic多partition的好处：数据冗余（备份安全，还有一个就是
可以把数据路由到不同的系统上去

streaming两种架构模式：
highlevelapi:在kafka的executor运行着一个reciver 开启线程去抽取一个分区的数据，如果有多个分区，reciver就开启多个线程，这种架构一个是不安全容易丢数据如果数据还没收到streaming就挂了是不可以的，虽然有wal预写入日志（事物型的），但是也不够好，还有一个就是吞吐量不高，自动提交offset


lowlevelapi:executor运行在，kafka的分区和rdd的分区对应，吞吐量比较高

streaming代码升级：





spark 运行 application 提交到结束的一整个流程:




回顾：

1、spark核心组件:

Driver功能:

main函数在driver执行的

spark任务在执行的过程中 是懒执行,会等待action算子 根据宽依赖 反向的划分stage

1个stage 就是一个taskset

一个stage 根据所处理的rdd的分区个数 决定了一个stage有多少个task，一个task 处理一个分区的数据

task调度也是由driver 完成的


申请资源：指的是executor的个数、executor所需内存的大小


Executor：一个jvm进程，执行driver 分配的task


Standalone client 模式:


当通过spark-submit脚本 提交任务的时候，首先会在本地 启动这个Driver
当driver 启动后会向master，注册我们的spark 应用程序

完后当master 接受到driver的注册请求会，driver 会向master 提出我们的资源申请

完后master 会获取到所有 至少能启动一个executor的所有worker
完后会遍历 所有能启动至少一个executor的worker,完后把资源分配给这些worker

资源分配完毕后 会根据资源分配情况，在对应的worker上启动相应数量的executor进程

executor进程启动后 会反向注册到driver上

总结：driver 是在本地提交机器上运行的，整个资源的调度是由master来操作的



Yarn-cluster 模式

当通过spark-sublimt或者 client(本地机器) 提交任务的时候 会向master注册我们的应用程序

完后master 会首先选择一个worker 节点 先去启动我们的Driver进程

当我们的Driver进程启动成功后，会向我们的master 申请executor 所需的资源

当资源申请完毕后，matser 会在其他worker 节点，启动我们相应数量的executor进程

当exexutor 进程启动完成后，会反向注册给driver

反向注册的目的是，是让driver 知道，每个worker节点有哪些可用的executor

之后对决定了 到底是哪个executor 执行哪个task（具体哪个task 找到哪个executor 后面会详细讲）



Spark 通讯架构 

。。。。。。。。

不是很重要，暂且不整理了....



SparkContext内核解析:


 在spark中由sparkcontext 负责与集群进行通讯、资源申请以及任务的分配和监控

sparkcontext 涵盖了所有driver的核心功能，driver 负责sparkcontext的 初始化以及关闭


sparkcontext包含三个核心组件：

dagscheduler：负责stage级别的调度
（当遇到action算子 ，根据宽依赖 反向划分stage、以及每个stage生成taskset都是由dagscheduler完成的）

taskscheduler：负责task级别的调度
（将dagscheduler根据job进行划分生成的每个taskset后，具体每个taskset中每个task 如何调度是由taskscheluder来负责的）

schedulerbackend：负责通讯、资源申请以及管理
（负责和集群管理器resourcemanager以及和executor进行通讯的）



内核级别的提交流程解析:



当driver启动之后 会执行main程序，注意:并不是说driver启动之后 就向resourcemanager 去申请资源的,在main函数里 会创建一个sparkconf，完后创建sparkcontext，当sparkcontext创建完成之后，我们的driver 才开始执行它的核心功能

在sparkcontext实例 初始化过程中，会生成 四个组件:
dagscheduler、taskscheduler、schedulerbackend、heartbeatreceiver
heartbeatreceiver负责接收 executor每隔一段时间发过来的心跳包，用来监控executor是否是处于一个运行状态

在上面组件初始化完成之后，就要通过schedulerbackend开始去和applicationMaster进行通讯在向yarn-resourcemanager 申请本次application运行过程中所需要的executor资源，当container 完成资源分配后，会由applicationmaster 去通知对应的nodemanager 去启动executor，当每个exexutor启动完成之后

executor 也有一个叫做executorbackend的东西，通过executorbackend 反向注册给sparkcontext中的schedulerbackend，反向注册完成之后，schedulerbackend就知道，对于我当前的application 有哪些可用的executor，当所有的反向注册完成之后，我的driver 才开始执行main程序，当遇到action算子之后，spark会启动job，根据dagscheduler划分stage、每个stage会有多个taskset，每个taskset会交给taskscheduler，那么taskscheduler会对taskset里的每一个 task进行调度，会分析哪一个task 具体去哪一个executor里执行，当确定之后，会通过schedulerbackend 将数据发送给executor，启动task，当所有的task 执行完毕后，driver 会关闭 sparkcontext


spark任务调度机制:

spark任务调度机制是指 遇到action算子之后的流程


在yarn-cluster 详细的 任务提交流程：


通过任务提交机器 也就是client会向我们的resouremanager 提交我的application

当我的resouremanager接收到application申请之后，会给我的client 返回一个响应

当我的client 接收到这个响应之后，会向resourcemanager 发送applicationmaster启动的上下文

那么发送给我们这个resourcemanager之后，resourcemanager 会给applicationmaster 分配 container

然后去通知 对应的nodemanager 启动 applicationmaster，当applicationmaster 启动成功后

会去启动driver，启动成功后，会给applicationmaster 一个启动成功的响应，这个启动成功的响应 是通过
schedulerbackend来做的

当applicationmaster 接收到 driver 启动成功的响应之后，明确了driver所需资源后，会向我的resourcemanager
去申请container，当资源到位后 resourcemanager 把它所分配的资源位置信息 告诉会给applicationmaster

applicationmaster 就会通知 对应的nodemanager 去启动对应的executor

当executor 启动完成之后，会通过它的executorbackend 向我的driver的 schedulerbackend进行一个注册

注册完成之后 ，开始执行我们的代码逻辑，然后遇到action算子 进行stage划分....巴拉巴拉一堆操作 完后根据相关算法

决定哪个task 分配给哪一个executor上去执行，具体算法会详细解释:

在exexutor 执行task的过程中，会不断给driver 返回task的状态




任务调度概述：

当遇到一个action算子 就会start 一个job


SparkStage级别的任务调度:

当遇到一个action算子的时候，会调用 sc.runjob去启动一个job
在runjob 代码内部会调用 dag.runJob

在dag.runJob内部调用dag.submitJob 在submitJob代码中会进行stage的划分,完后会对划分出来的每个stage调用submitStage
(这里有个细节就是，会优先提交最后一个job的stage俗称resultStage到submitstage方法里面,前边所有的stage称之为shufflemapstage，完后判断刚刚准备要提交的resultstage还有没有没提交的父stage，如果有就优先找到父stage，然后尝试在提交父stage，依次推导到最前面的stage 发现没有父stage了，然后在开始从前往后提交其他stage) 提交完后 会调用dag.submitMissingTasks 在此方法中完成taskset的创建,
完后调用taskscheduler.submitTasks



一个stage对应一个taskset 一个taskset 对应多个task


Spark task级别的调度:


dagscheduler将stage 打包提交给taskscheduler，taskscheduler 将每一个taskset 封装成tasksetmanager中、
在这个tasksetmanager 有我们taskset的 完成数据信息 以及其他额外的配置信息

具体代码流程如下:
当taskscheduler.submitTasks 的时候 会在内部调用createTaskSetManager
完成将每一个taskset进行tasksetmanager的封装  完后会调用addTaskSetManager将每个tasksetmanager 放入一个tasksetpool的池子中去后

会调用schedulerbackend的reviveoffers的方法 会给driver的endpoint
发送一个信息，当driver收到reviveoffers信息后 会调用makeOffers方法

这个 makeoffers 会获取到所有能够使用刚才方向注册过来的可用的executor的信息


然后会调用 taskscheduler中的resourceoffers 在起内部会调用getsortedtasksetqueue会从池子里 往外拿tasksetmanager
然后 针对 每个tasksetmanager 和刚才收到的executor 完成任务的一个分配



如何从pool 中拿tasksetmanager（任务调度策略）


pool: 结构 树形结构

根节点是rootPOOl

下面是 自定义的各个pool

叶子结点是一个一个的 等待调度的tasksetmanager

 pool 和tasksetmanager 都继承了相同的父类schedulable，就意味着 他们可能会有相同的方法以及属性


 对于task 级别的调度 总共有两种调度策略：

 1、FIFO ：虽然起这个名字 并不是先进先出的 当选择 fifo模式的时候，就会影响这个树的结构，这个树只包含rootpool

 并不会有子节点，下面直接连着 tasksetmanager

 每一个 tasksetmanager 会有一个 priority属性，值越小，优先级越高 就会越优先的被调度


 如果优先级别相同 在去比较stage的身份，让靠在前面的stage 优先去从池子里调度tasksetmanager


 2、Fair：公平调度策略

 一个taskset 里会有多个task，而每个task可能会有不同的状态（task 正在运行状态、task错误运行状态等）

 当选择fair调度策略 池子的树形结构是样的：

 最上层 是一个rootpool、子节点是一个一个的 自定义的pool，自定义pool的叶子结点是一个个等待调度的tasksetmanager


 fair 调度策略是先排序分支下的pool执行，完后在对tasksetmanager进行优先级的排序
 如果在sparkcontext 没有指定子pool 会将所有的tasksetmanager 放到一个叫做default pool的池子中去

排序的指标 参照三个值：

1、runingtasks：当前pool 或者是tasksetmanager里正在运行的task个数

2、minshare:可以在公平调度配置文件fairscheduler.xml中被指定

3、weight:可以在公平调度配置文件fairscheduler.xml中被指定


1）如果a对象(pool或者tasksetmanager)的runningtasks大于minshare，b对象的runningtasks小于minshare，那么b排在a的前面

也就是说让正在运行比较少的runningtasks的pool 优先执行

2）如果a对象和b对象都小于他们各自的minshare，则求runningtasks和minshare的比值，谁小谁排在前面

也就是说minshare 使用率低的先执行

3）如果runningtasks 都大于minshare，则比较runningtasks 和weight的比值，谁小谁排在前面

也就是说weight使用率低的先执行




 本地化调度原则:

 我们现在已经知道如何去决定pool里哪一个tasksetmanager优先先执行，这时候就涉及到我怎么决定taskmanager里的哪一个task要到哪一个executor里去执行的问题  这就要看本地化调度了


 1、process_local:进程本地化（我在向我的executor分配task的时候优先选择数据所在的executor去执行我的task，）当这个executor内存不够 隔一段时间还不行的话 就会自动降级

 降到node_local：节点本地化,(两个executor在一台机器上) 如果还不行 就会继续降级到
 rack_local:机架本地化
 


 shuffe 解析:
 map端 task的个数准确的说是由第一个读取数据源的rdd 的分区个数决定

 reduce端task的个数由什么决定呢，一会会详细的讲解


 map端 task的个数的确定：

 spark.default.parallelism = max(所有executor使用的core总数，2)  (默认值)


 spark.files.maxpartitionBytes = 128M (一个分区能存储的数据最大字节数)


生成sparkcontext的时候 同时会生成两个参数:

sc.defaultparallelism = spark.default.parallelism
sc.defaultMinPartitions = min(spark.default.parallelism,2)

当以上参数确定后，就可以推算rdd的分区数了


1、通过scala 集合方式生成rdd：

val rdd = sc.parallelize(1 to 10)

rdd的分区数：sc.defaultparallelism

2、在本地文件系统通过textFile方式生成的RDD

val rdd = sc.textFile(“path/file”)

rdd的分区数 = max（本地file的分片数， sc.defaultMinPartitions）

3、在HDFS文件系统生成的RDD

rdd的分区数 = max（HDFS文件的Block数目， sc.defaultMinPartitions）

4、从HBase数据表获取数据并转换为RDD

rdd的分区数 = Table的region个数

通过获取json（或者parquet等等）文件转换成的DataFrame

rdd的分区数 = 该文件在文件系统中存放的Block数目

 5、Spark Streaming获取Kafka消息对应的分区数


在Receiver的方式中:
 Spark中的partition和kafka中的partition并不是相关的，所以如果我们加大每个topic的partition数量，
仅仅是增加线程来处理由单一Receiver消费的主题。但是这并没有增加Spark在处理数据上的并行度。


基于DirectDStream:
Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从Kafka中读取数据，所以在Kafka partition和RDD partition之间，有一个一对一的映射关系。




Reduce端task个数的确定:

1、可以通过算子里的参数决定

2、取决于 最后一个rdd的分区个数，决定了reduce端task的个数



问题：map端的stage 和reduce端的stage 可能会在不同的时间被调度执行，也就意味着这两个stage各自的task 可能会在不同的executor上被执行，那么这样就会出现一个问题：
reduce端的task 如何确定我所对应的map task它生成的文件会在什么位置呢？

spark是这样解决的

1、当map端的task执行完毕后，executor会通过mapputputtrackerworker 将生成的磁盘文件信息以及位置信息封装到一个叫mapstatus的结构里 完后发送给driver进程的mapoutputtrackermaster对象

2、当reduce 端task开始执行的时候，会先让本进程的mapputputtrackerworker 向driver的mapoutputtrackermaster发起磁盘小文件的位置信息的请求



未优化的hashshuffle：

每个maptask 都会生成和reducetask 相同的磁盘文件，如果有1000个maptask、3个reduce task，就会生成3*1000个小的磁盘文件，会造成磁盘io瓶颈和 网络ios瓶颈


优化的hashshuffle：

每个maptask 复用之前的 内存缓冲区，和复用相同的磁盘空间


spark 正在使用的Sortshuffle：

普通机制：
先把数据写到内存缓冲区中、当内存缓冲区达到域值之后,会往外溢写，会根据reduce task的个数，开辟相应个数的额外的内存缓冲区，然后根据key的hashcode 决定到底进入到哪一个额外的内存缓冲区中
并且在刚才开辟的内存缓冲区中，对数据进行排序

然后在该开辟的内存缓冲中以每次10000条的数据去向我的磁盘文件进行刷写，最后按顺序将这些小的磁盘文件拼接到一个大的磁盘文件当中去（同时会生成一个索引文件，记录了某一块数据到哪一块的数据 是属于reduce task的） 这样做的好处就是速度快

byPass机制:
实际就是不引起排序过程的一个普通机制的Sortshuffle
当shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold值的时候就不尽排序过程了















